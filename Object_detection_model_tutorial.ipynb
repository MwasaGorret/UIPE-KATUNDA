{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_detection_model_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MwasaGorret/UIPE-KATUNDA/blob/master/Object_detection_model_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo6x8C09CUxR"
      },
      "source": [
        "# Object Detection Model Tutorial\n",
        "prerequisites:\n",
        "1. A basic understanding of python programming and deep learning.\n",
        "\n",
        "Check out this [tutorial](https://makmlclub.github.io/colab.html) to get familiar with Colab, and deep learning basics.\n",
        "\n",
        "---\n",
        "Notes on Object detection: </br>\n",
        "[Object detection](https://en.wikipedia.org/wiki/Object_detection) is a machine learning technique under computer vision that involves locating the presence of objects with a bounding box and types or classes of the located objects in an image.\n",
        "*   Input: An image with one or more objects, such as a photograph.\n",
        "*   Output: One or more bounding boxes (e.g. defined by a point, width, and height), and a class label for each bounding box.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this tutorial, we shall use the [Tensorflow Object Detection API](https://https://github.com/tensorflow/models/tree/master/research/object_detection), however,there are a number of object detection models in existence that you could use not to mention you can build one for yourself from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05OYHs_sS_Rr"
      },
      "source": [
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwGnqsh3DmFU"
      },
      "source": [
        "!mkdir object_detection\n",
        "os.chdir('object_detection/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2karAqisZ1rV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1daa73e-acf7-460d-978b-6a6f1db190fa"
      },
      "source": [
        "!git clone --depth=1 https://github.com/tensorflow/models.git\n",
        "\n",
        "# TF-Slim is a lightweight library for defining, training and evaluating complex models in TensorFlow\n",
        "!pip install tf_slim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 2305, done.\u001b[K\n",
            "remote: Counting objects: 100% (2305/2305), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2000/2000), done.\u001b[K\n",
            "remote: Total 2305 (delta 562), reused 953 (delta 282), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2305/2305), 30.60 MiB | 24.55 MiB/s, done.\n",
            "Resolving deltas: 100% (562/562), done.\n",
            "Collecting tf_slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdxzy6-YbIIY"
      },
      "source": [
        "## Protobuf installation/Compilation\n",
        "Tensorflow Object Detection API uses Protobufs to configure model and training parameters. The Protobuf libraries must be installed to use the framework.\n",
        "\n",
        "Notes on Protobufs:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Protobufs*, or a serialized protocol buffer is a portable, extensible and efficient binary format developed by Google. Protobufs are defined by a simple language format (similar in spirit to a `C/C++ struct`) as defined below:\n",
        "```\n",
        "syntax: \"proto3\"\n",
        "message Person {\n",
        "  string name: 1;\n",
        "  int32 id: 2;\n",
        "  repeated string email:3;\n",
        "}\n",
        "```\n",
        "This definition specifies the use of version 3 of `protobuf`, and specifies the each `Person` object may optionally have a `name` of type `string`, an `id` of `int32` and an `email` of type `string`. The numbers 1 to 3 are field identifiers used in each records binary representation. This definition can be saved in `.proto` file and compiled.\n",
        "\n",
        "## Tensorflow Protobufs\n",
        "Tensorflow primarily uses the `Example` protobuf, which represents a single instance in a dataset. It contains a list of named features, with each feature being either a list of byte strings, a list of floats, or a list of floats. An example is shown below:\n",
        "```\n",
        "syntax = \"proto3\";\n",
        "message BytesList { repeated bytes value = 1; }\n",
        "message FloatList { repeated float value = 1 [packed = true]; }\n",
        "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
        "message Feature {\n",
        "  oneof kind {\n",
        "    BytesList bytes_list = 1;\n",
        "    FloatList float_list = 2;\n",
        "    Int64List int64_list = 3;\n",
        "    }\n",
        "};\n",
        "message Features { map<string, Feature> feature = 1; };\n",
        "message Example { Features features = 1; };\n",
        "```\n",
        "`[packed = true]` is used for repeated numerical fields to enusre efficient encoding.\n",
        "\n",
        "---\n",
        "\n",
        "Further reading:\n",
        "1. [https://homl.info/protobuf](https://homl.info/protobuf)\n",
        "2. Chapter 13: Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow by `Aurélien Géron`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7whJn2az0sP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d89515-91a2-48ae-bb16-4e7607ba5e95"
      },
      "source": [
        "%cd models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "pwd = os.getcwd()\n",
        "\n",
        "# Add TF_slim to the system path.\n",
        "os.environ['PYTHONPATH'] += f':{pwd}:{pwd}/slim'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/object_detection/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnWFhYGPDAqj"
      },
      "source": [
        "## Adding necessary Environment Variables\n",
        "Install the required packages from the `object_dection/models/research`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r22LNm2ZOHyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc9aeae-0290-4f7a-ba6e-89e9ba833e76"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/object_detection/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZPryb69bffo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f06dd75-c337-47d7-963a-35097a3f07ed"
      },
      "source": [
        "# Get the setup file\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "\n",
        "# Installs from the setup.py file\n",
        "!pip install ."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/object_detection/models/research\n",
            "Collecting avro-python3\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/5a/819537be46d65a01f8b8c6046ed05603fb9ef88c663b8cca840263788d58/avro-python3-1.10.0.tar.gz\n",
            "Collecting apache-beam\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/3f/93816e989e8e59b337f22927778494a99b2a3e78a3b6a9e34d043c6fab4e/apache_beam-2.25.0-cp36-cp36m-manylinux2010_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 376kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.29.21)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Collecting lvis\n",
            "  Downloading https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.1.4)\n",
            "Collecting tf-models-official\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/33/91e5e90e3e96292717245d3fe87eb3b35b07c8a2113f2da7f482040facdb/tf_models_official-2.3.0-py2.py3-none-any.whl (840kB)\n",
            "\u001b[K     |████████████████████████████████| 849kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.7.4.3)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hCollecting pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2018.9)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 51.7MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/a9/473ef678c8862d74c63e11d14afbdbeabe67f92fedd82405de5337d7e6de/fastavro-1.2.0-cp36-cp36m-manylinux2014_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.33.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.12.4)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2.8.1)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-slim->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->object-detection==0.1) (50.3.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.6/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.21.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.5.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (3.13)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/e9/57d869561389884136be65a2d1bc038fe50171e2ba348fda269a4aab8032/opencv_python_headless-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (36.7MB)\n",
            "\u001b[K     |████████████████████████████████| 36.7MB 87kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.7)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.7.12)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.8.3)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official->object-detection==0.1) (0.1.5)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.41.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (0.0.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (20.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (0.24.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.35.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.17.2)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-official->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-official->object-detection==0.1) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official->object-detection==0.1) (1.52.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.4.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.1.0)\n",
            "Building wheels for collected packages: object-detection, avro-python3, hdfs, future, dill, py-cpuinfo\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-cp36-none-any.whl size=1599128 sha256=073d5b01426b52e1ba387f5b1069a909f84e39e96f89e5558601cf3feadfb829\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gc7ailq4/wheels/cd/38/29/a5471282c15ca3a4f95b2f77066e3a7154685b6a428f65cd5d\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.0-cp36-none-any.whl size=43735 sha256=9bde7f634ab5f709afe3b940a905250a5010dc40f784fda484bd6dd07ff7dc16\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/15/cd/fe4ec8b88c130393464703ee8111e2cddebdc40e1b59ea85e9\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33213 sha256=ac391f478df8698533f291341ab0e616d675e9f4232cc78820652d645ee2aa58\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=c75664554f17d7ee9dbdf84584badaea8f04bd39a38ce46405abde8478e68b94\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=acb187d66c5bf48dbdec7797887b70be1175eea1b646f003d96458c11f5dab50\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20071 sha256=f1a54a651ec58b8ddc14b9c24c40da3e27810b29eef54d8e7400a2e2ccf453b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built object-detection avro-python3 hdfs future dill py-cpuinfo\n",
            "\u001b[31mERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: apache-beam 2.25.0 has requirement avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\", but you'll have avro-python3 1.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: avro-python3, requests, hdfs, pbr, mock, pyarrow, future, dill, fastavro, apache-beam, lvis, tensorflow-model-optimization, opencv-python-headless, py-cpuinfo, sentencepiece, tf-models-official, object-detection\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: dill 0.3.3\n",
            "    Uninstalling dill-0.3.3:\n",
            "      Successfully uninstalled dill-0.3.3\n",
            "Successfully installed apache-beam-2.25.0 avro-python3-1.10.0 dill-0.3.1.1 fastavro-1.2.0 future-0.18.2 hdfs-2.5.8 lvis-0.5.3 mock-2.0.0 object-detection-0.1 opencv-python-headless-4.4.0.46 pbr-5.5.1 py-cpuinfo-7.0.0 pyarrow-0.17.1 requests-2.25.0 sentencepiece-0.1.94 tensorflow-model-optimization-0.5.0 tf-models-official-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIgsz7--5H5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3f9be6-907b-4f65-b70d-5e56f96469a3"
      },
      "source": [
        "# Test the setup\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-20 13:35:54.381880: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "2020-11-20 13:35:57.104793: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-20 13:35:57.161699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:35:57.162360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-20 13:35:57.162410: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-20 13:35:57.419155: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-20 13:35:57.533020: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-20 13:35:57.585284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-20 13:35:57.834481: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-20 13:35:57.896027: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-20 13:35:58.413131: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-20 13:35:58.413382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:35:58.414460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:35:58.415311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-20 13:35:58.482926: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-20 13:35:58.483164: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2758bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-20 13:35:58.483196: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-20 13:35:58.626530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:35:58.627261: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2758d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-20 13:35:58.627292: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-20 13:35:58.628253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:35:58.628859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-20 13:35:58.628905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-20 13:35:58.628951: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-20 13:35:58.628975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-20 13:35:58.628997: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-20 13:35:58.629023: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-20 13:35:58.629045: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-20 13:35:58.629067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-20 13:35:58.629146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:35:58.629772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:35:58.630312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-20 13:35:58.632747: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-20 13:36:02.463375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-20 13:36:02.463430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-20 13:36:02.463442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-20 13:36:02.467476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:36:02.468239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-20 13:36:02.468862: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-20 13:36:02.468910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 10.42s\n",
            "I1120 13:36:07.311876 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 10.42s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I1120 13:36:07.313301 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "I1120 13:36:07.350990 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "I1120 13:36:07.374225 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "I1120 13:36:07.398526 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.16s\n",
            "I1120 13:36:07.561284 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "I1120 13:36:07.718173 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.16s\n",
            "I1120 13:36:07.882189 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "I1120 13:36:08.043252 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.17s\n",
            "I1120 13:36:08.213150 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "I1120 13:36:08.261316 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I1120 13:36:08.585482 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I1120 13:36:08.585678 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 64\n",
            "I1120 13:36:08.585779 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 3\n",
            "I1120 13:36:08.592884 139973213132672 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 13:36:08.621181 139973213132672 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 13:36:08.621321 139973213132672 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 13:36:08.703801 139973213132672 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 13:36:08.703960 139973213132672 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 13:36:08.923208 139973213132672 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 13:36:08.923405 139973213132672 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 13:36:09.144637 139973213132672 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 13:36:09.144810 139973213132672 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 13:36:09.479709 139973213132672 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 13:36:09.479876 139973213132672 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 13:36:09.813413 139973213132672 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 13:36:09.813621 139973213132672 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 13:36:10.390448 139973213132672 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 13:36:10.390636 139973213132672 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1120 13:36:10.484722 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1120 13:36:10.525615 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 13:36:10.624331 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I1120 13:36:10.624521 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 88\n",
            "I1120 13:36:10.624612 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 4\n",
            "I1120 13:36:10.630353 139973213132672 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 13:36:10.656243 139973213132672 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 13:36:10.656368 139973213132672 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 13:36:10.817072 139973213132672 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 13:36:10.817212 139973213132672 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 13:36:11.147901 139973213132672 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 13:36:11.148082 139973213132672 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 13:36:11.475283 139973213132672 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1120 13:36:11.475456 139973213132672 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 13:36:11.907033 139973213132672 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1120 13:36:11.907235 139973213132672 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 13:36:12.328892 139973213132672 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1120 13:36:12.329065 139973213132672 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 13:36:12.869781 139973213132672 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1120 13:36:12.869988 139973213132672 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1120 13:36:13.076965 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1120 13:36:13.123925 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 13:36:13.227816 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I1120 13:36:13.227999 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 112\n",
            "I1120 13:36:13.228078 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 5\n",
            "I1120 13:36:13.233999 139973213132672 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 13:36:13.257980 139973213132672 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1120 13:36:13.258112 139973213132672 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 13:36:13.590081 139973213132672 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1120 13:36:13.590256 139973213132672 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 13:36:13.906349 139973213132672 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1120 13:36:13.906531 139973213132672 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 13:36:14.218911 139973213132672 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 13:36:14.219094 139973213132672 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1120 13:36:14.652055 139973213132672 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1120 13:36:14.652242 139973213132672 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1120 13:36:15.072697 139973213132672 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1120 13:36:15.072882 139973213132672 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1120 13:36:15.609893 139973213132672 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1120 13:36:15.610073 139973213132672 efficientnet_model.py:148] round_filter input=320 output=352\n",
            "I1120 13:36:15.814895 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=1408\n",
            "I1120 13:36:15.857258 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 13:36:15.964649 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I1120 13:36:15.964866 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 160\n",
            "I1120 13:36:15.964973 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 6\n",
            "I1120 13:36:15.975227 139973213132672 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1120 13:36:16.002375 139973213132672 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1120 13:36:16.002519 139973213132672 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 13:36:16.173310 139973213132672 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 13:36:16.173490 139973213132672 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 13:36:16.507389 139973213132672 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 13:36:16.507559 139973213132672 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 13:36:16.834911 139973213132672 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1120 13:36:16.835087 139973213132672 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1120 13:36:17.365592 139973213132672 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1120 13:36:17.365779 139973213132672 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1120 13:36:18.111719 139973213132672 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1120 13:36:18.111893 139973213132672 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1120 13:36:18.766319 139973213132672 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1120 13:36:18.766501 139973213132672 efficientnet_model.py:148] round_filter input=320 output=384\n",
            "I1120 13:36:18.975512 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=1536\n",
            "I1120 13:36:19.014637 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 13:36:19.126563 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I1120 13:36:19.126752 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 224\n",
            "I1120 13:36:19.126832 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1120 13:36:19.132370 139973213132672 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 13:36:19.156625 139973213132672 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 13:36:19.156741 139973213132672 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 13:36:19.316395 139973213132672 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 13:36:19.316570 139973213132672 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 13:36:19.752106 139973213132672 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1120 13:36:19.752322 139973213132672 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1120 13:36:20.182059 139973213132672 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1120 13:36:20.182236 139973213132672 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1120 13:36:20.825399 139973213132672 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1120 13:36:20.825607 139973213132672 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1120 13:36:21.466033 139973213132672 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1120 13:36:21.466207 139973213132672 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1120 13:36:22.324209 139973213132672 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1120 13:36:22.324380 139973213132672 efficientnet_model.py:148] round_filter input=320 output=448\n",
            "I1120 13:36:22.527367 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=1792\n",
            "I1120 13:36:22.570219 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 13:36:22.688632 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I1120 13:36:22.688827 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 288\n",
            "I1120 13:36:22.688907 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1120 13:36:22.694879 139973213132672 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 13:36:22.724183 139973213132672 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1120 13:36:22.724329 139973213132672 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 13:36:23.228892 139973213132672 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1120 13:36:23.229079 139973213132672 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 13:36:23.775785 139973213132672 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 13:36:23.775961 139973213132672 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1120 13:36:24.325938 139973213132672 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1120 13:36:24.326119 139973213132672 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1120 13:36:25.085750 139973213132672 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1120 13:36:25.085941 139973213132672 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1120 13:36:25.840698 139973213132672 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1120 13:36:25.840875 139973213132672 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1120 13:36:26.824136 139973213132672 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1120 13:36:26.824317 139973213132672 efficientnet_model.py:148] round_filter input=320 output=512\n",
            "I1120 13:36:27.150835 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=2048\n",
            "I1120 13:36:27.191337 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 13:36:27.330187 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I1120 13:36:27.330357 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1120 13:36:27.330438 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1120 13:36:27.337595 139973213132672 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1120 13:36:27.364370 139973213132672 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1120 13:36:27.364503 139973213132672 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 13:36:27.618096 139973213132672 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 13:36:27.618276 139973213132672 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 13:36:28.275729 139973213132672 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1120 13:36:28.275907 139973213132672 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1120 13:36:28.938474 139973213132672 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1120 13:36:28.938669 139973213132672 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1120 13:36:30.117341 139973213132672 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1120 13:36:30.117530 139973213132672 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1120 13:36:30.973055 139973213132672 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1120 13:36:30.973235 139973213132672 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1120 13:36:32.161751 139973213132672 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1120 13:36:32.161932 139973213132672 efficientnet_model.py:148] round_filter input=320 output=576\n",
            "I1120 13:36:32.478764 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=2304\n",
            "I1120 13:36:32.518506 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1120 13:36:32.673649 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I1120 13:36:32.673832 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1120 13:36:32.673913 139973213132672 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1120 13:36:32.680017 139973213132672 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1120 13:36:32.706027 139973213132672 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1120 13:36:32.706148 139973213132672 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 13:36:33.054965 139973213132672 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1120 13:36:33.055144 139973213132672 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1120 13:36:33.821629 139973213132672 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1120 13:36:33.821799 139973213132672 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1120 13:36:34.579700 139973213132672 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1120 13:36:34.579884 139973213132672 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1120 13:36:35.663731 139973213132672 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1120 13:36:35.663909 139973213132672 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1120 13:36:36.748559 139973213132672 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1120 13:36:36.748761 139973213132672 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1120 13:36:38.553437 139973213132672 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1120 13:36:38.553704 139973213132672 efficientnet_model.py:148] round_filter input=320 output=640\n",
            "I1120 13:36:38.986411 139973213132672 efficientnet_model.py:148] round_filter input=1280 output=2560\n",
            "I1120 13:36:39.026171 139973213132672 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 30.94s\n",
            "I1120 13:36:39.204755 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 30.94s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I1120 13:36:39.213117 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I1120 13:36:39.215501 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I1120 13:36:39.216269 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I1120 13:36:39.218255 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I1120 13:36:39.220136 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I1120 13:36:39.220832 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I1120 13:36:39.222079 139973213132672 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 20 tests in 42.332s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4B260eTKjp1"
      },
      "source": [
        "Now the environment is setup and ready for use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-36uD1DZKsne"
      },
      "source": [
        "# Setting up the workspace\n",
        "The pipeline to train an object detector involves a couple of steps that we shall go through one at a time.\n",
        "\n",
        "1. Organize the workspace/training files\n",
        "3. Generate tf records from the datasets\n",
        "4. Configure a simple training pipeline\n",
        "5. Train a model and monitor it’s progress\n",
        "6. Export the resulting model and use it to detect objects.\n",
        "\n",
        "In this tutorial, we use an already prepared and annotated dataset, the labelling was done using [Labelbox](https://labelbox.com) and the annotations extracted as a [JSON](https://www.tutorialspoint.com/json/index.htm) file. That step is skipped in this tutorial, however there is a number of open source resoures to used for data annotation including:\n",
        "1. [LabelImg](https://github.com/tzutalin/labelImg)\n",
        "2. [VGG Image Annotator](http://www.robots.ox.ac.uk/~vgg/software/via/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x74pAKgNNapk"
      },
      "source": [
        "At this stage, our directory should be as below;\n",
        "\n",
        "\n",
        "```\n",
        "object_detection\n",
        "├─ models\n",
        "│   ├─ community\n",
        "│   ├─ research\n",
        "│   └─ etc\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCzYM08YO0k-"
      },
      "source": [
        "## 1. Organise workspace/training files.\n",
        "Create a folder under `object_detection` named `workspace` and under it create the `training` folder.\n",
        "\n",
        "```\n",
        "object_detection\n",
        "├─ models\n",
        "│   ├─ official\n",
        "│   ├─ research\n",
        "│   ├─ samples\n",
        "│   └─ tutorials\n",
        "└─ workspace\n",
        "    └─ data\n",
        "```\n",
        "The data folder contains all the files related to training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3aRNtxPwb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70519980-6f38-42aa-a341-05d6aef3e36f"
      },
      "source": [
        "os.chdir('../../')\n",
        "os.listdir()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['models']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WstuuOseYN1j"
      },
      "source": [
        "!mkdir workspace\n",
        "!mkdir workspace/data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2p-uq-Rcw8"
      },
      "source": [
        "The data folder structure is as defined below:\n",
        "```\n",
        "data\n",
        "├─ annotations\n",
        "├─ images\n",
        "│   ├─ test\n",
        "│   └─ train\n",
        "├─ pre-trained-model\n",
        "└─ training\n",
        "```\n",
        "Notes on training structure folder:\n",
        "\n",
        "---\n",
        "\n",
        "`annotations`: This folder stores all \\*.csv files and the respective TensorFlow \\*.record files, which contain the list of annotations for dataset images.\n",
        "\n",
        "`images`: This folder contains a copy of all the images in our dataset.\n",
        "\n",
        "`images\\train`: This folder contains a copy of all images.\n",
        "\n",
        "`images\\test`: This folder contains a copy of all images.\n",
        "\n",
        "`pre-trained-model`: This folder will contain the pre-trained model of your choice, which shall be used as a starting checkpoint for the training job.\n",
        "\n",
        "`training`: This folder will contain the training pipeline configuration file \\*.config, as well as a \\*.pbtxt label map file and all files generated during the training of our model.\n",
        "\n",
        "---\n",
        "\n",
        "Before adding the data to the respective folders, we shall prepare it i.e. set it to the format compatible for use with `Tensorflow Object Detection API`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKebdxIpT-8t"
      },
      "source": [
        "!mkdir workspace/data/images \n",
        "!mkdir workspace/data/images/train\n",
        "!mkdir workspace/data/images/test\n",
        "!mkdir workspace/data/pre-trained-model\n",
        "!mkdir workspace/data/training\n",
        "!mkdir workspace/data/annotations"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ckzx9c2Xc_a"
      },
      "source": [
        "### Data Preparation\n",
        "Upload the target dataset in a compressed format. The file should include the images and the annotations for each image in a unified JSON file.\n",
        "\n",
        "```\n",
        "dataset\n",
        "├─ labels.json\n",
        "└─ images\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UKPTvyVQGLD"
      },
      "source": [
        "# TODO\n",
        "# 1. Upload the dataset file to the /object_detection directory.\n",
        "# 2. Unzip the dataset file.\n",
        "\n",
        "# NOTES: The dataset can be uploaded anywhere, as long as you keep track of the where."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mJQJNZe2Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed2eba2-c0a2-4693-c439-256505af66aa"
      },
      "source": [
        "!unzip /content/dataset.zip "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/labels.json     \n",
            "   creating: dataset/images/\n",
            "  inflating: dataset/images/img_117.jpg  \n",
            "  inflating: dataset/images/img_423.jpg  \n",
            "  inflating: dataset/images/img_363.jpg  \n",
            "  inflating: dataset/images/img_475.jpg  \n",
            "  inflating: dataset/images/img_55.jpg  \n",
            "  inflating: dataset/images/img_448.jpg  \n",
            "  inflating: dataset/images/img_176.jpg  \n",
            "  inflating: dataset/images/img_140.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114319_212.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_170753_308.jpg  \n",
            "  inflating: dataset/images/img_85.jpg  \n",
            "  inflating: dataset/images/img_543.jpg  \n",
            "  inflating: dataset/images/img_66.jpg  \n",
            "  inflating: dataset/images/img_73.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155600_637.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164024_837.jpg  \n",
            "  inflating: dataset/images/img_364.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114249_798.jpg  \n",
            "  inflating: dataset/images/img_425.jpg  \n",
            "  inflating: dataset/images/img_412.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115144_771.jpg  \n",
            "  inflating: dataset/images/img_473.jpg  \n",
            "  inflating: dataset/images/img_502.jpg  \n",
            "  inflating: dataset/images/img_351.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113128_862.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115401_896.jpg  \n",
            "  inflating: dataset/images/img_194.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115003_513.jpg  \n",
            "  inflating: dataset/images/img_95.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163454_874.jpg  \n",
            "  inflating: dataset/images/img_479.jpg  \n",
            "  inflating: dataset/images/img_570.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165241_702.jpg  \n",
            "  inflating: dataset/images/img_332.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162342_222.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162520_237.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160210_788.jpg  \n",
            "  inflating: dataset/images/img_480.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114751_028.jpg  \n",
            "  inflating: dataset/images/img_410.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164443_762.jpg  \n",
            "  inflating: dataset/images/img_300.jpg  \n",
            "  inflating: dataset/images/img_122.jpg  \n",
            "  inflating: dataset/images/img_139.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_180415_667.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165046_641.jpg  \n",
            "  inflating: dataset/images/img_401.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112725_143.jpg  \n",
            "  inflating: dataset/images/img_183.jpg  \n",
            "  inflating: dataset/images/img_339.jpg  \n",
            "  inflating: dataset/images/img_450.jpg  \n",
            "  inflating: dataset/images/img_249.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112029_058.jpg  \n",
            "  inflating: dataset/images/img_170.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155938_010.jpg  \n",
            "  inflating: dataset/images/img_350.jpg  \n",
            "  inflating: dataset/images/img_11.jpg  \n",
            "  inflating: dataset/images/img_48.jpg  \n",
            "  inflating: dataset/images/img_61.jpg  \n",
            "  inflating: dataset/images/img_104.jpg  \n",
            "  inflating: dataset/images/img_337.jpg  \n",
            "  inflating: dataset/images/img_537.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115010_402.jpg  \n",
            "  inflating: dataset/images/img_211.jpg  \n",
            "  inflating: dataset/images/img_238.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112921_118.jpg  \n",
            "  inflating: dataset/images/img_180.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120407_231.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160329_423.jpg  \n",
            "  inflating: dataset/images/img_189.jpg  \n",
            "  inflating: dataset/images/img_214.jpg  \n",
            "  inflating: dataset/images/img_413.jpg  \n",
            "  inflating: dataset/images/img_24.jpg  \n",
            "  inflating: dataset/images/img_17.jpg  \n",
            "  inflating: dataset/images/img_470.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155655_601.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164232_882.jpg  \n",
            "  inflating: dataset/images/img_524.jpg  \n",
            "  inflating: dataset/images/img_569.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165358_793.jpg  \n",
            "  inflating: dataset/images/img_119.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163316_637.jpg  \n",
            "  inflating: dataset/images/img_273.jpg  \n",
            "  inflating: dataset/images/img_531.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163143_312.jpg  \n",
            "  inflating: dataset/images/img_103.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112310_493.jpg  \n",
            "  inflating: dataset/images/img_53.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114607_805.jpg  \n",
            "  inflating: dataset/images/img_44.jpg  \n",
            "  inflating: dataset/images/img_193.jpg  \n",
            "  inflating: dataset/images/img_321.jpg  \n",
            "  inflating: dataset/images/img_561.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155255_327.jpg  \n",
            "  inflating: dataset/images/img_389.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165315_799.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163201_261.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112908_023.jpg  \n",
            "  inflating: dataset/images/img_466.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163608_413.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164925_494.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163224_851.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_170738_872.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165210_248.jpg  \n",
            "  inflating: dataset/images/img_159.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112806_958.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114436_235.jpg  \n",
            "  inflating: dataset/images/img_418.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155711_383.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162943_980.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120141_959.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115502_500.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171413_599.jpg  \n",
            "  inflating: dataset/images/img_467.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114830_420.jpg  \n",
            "  inflating: dataset/images/img_406.jpg  \n",
            "  inflating: dataset/images/img_141.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114617_567.jpg  \n",
            "  inflating: dataset/images/img_212.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163332_377.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113501_401.jpg  \n",
            "  inflating: dataset/images/img_383.jpg  \n",
            "  inflating: dataset/images/img_27.jpg  \n",
            "  inflating: dataset/images/img_230.jpg  \n",
            "  inflating: dataset/images/img_481.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115815_277.jpg  \n",
            "  inflating: dataset/images/img_525.jpg  \n",
            "  inflating: dataset/images/img_520.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162400_047.jpg  \n",
            "  inflating: dataset/images/img_38.jpg  \n",
            "  inflating: dataset/images/img_424.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171501_366.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171236_371.jpg  \n",
            "  inflating: dataset/images/img_226.jpg  \n",
            "  inflating: dataset/images/img_523.jpg  \n",
            "  inflating: dataset/images/img_20.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113848_560.jpg  \n",
            "  inflating: dataset/images/img_496.jpg  \n",
            "  inflating: dataset/images/img_562.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115302_925.jpg  \n",
            "  inflating: dataset/images/img_136.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114956_844.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163310_105.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163054_595.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163048_857.jpg  \n",
            "  inflating: dataset/images/img_272.jpg  \n",
            "  inflating: dataset/images/img_514.jpg  \n",
            "  inflating: dataset/images/img_275.jpg  \n",
            "  inflating: dataset/images/img_361.jpg  \n",
            "  inflating: dataset/images/img_175.jpg  \n",
            "  inflating: dataset/images/img_234.jpg  \n",
            "  inflating: dataset/images/img_206.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113452_826.jpg  \n",
            "  inflating: dataset/images/img_106.jpg  \n",
            "  inflating: dataset/images/img_98.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120146_641.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115120_819.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120331_882.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114804_493.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162913_204.jpg  \n",
            "  inflating: dataset/images/img_542.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120129_361.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112405_553.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120321_656.jpg  \n",
            "  inflating: dataset/images/img_580.jpg  \n",
            "  inflating: dataset/images/img_476.jpg  \n",
            "  inflating: dataset/images/img_453.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114931_098.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163718_544.jpg  \n",
            "  inflating: dataset/images/img_509.jpg  \n",
            "  inflating: dataset/images/img_150.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165511_465.jpg  \n",
            "  inflating: dataset/images/img_386.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113114_243.jpg  \n",
            "  inflating: dataset/images/img_338.jpg  \n",
            "  inflating: dataset/images/img_144.jpg  \n",
            "  inflating: dataset/images/img_72.jpg  \n",
            "  inflating: dataset/images/img_533.jpg  \n",
            "  inflating: dataset/images/img_478.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155819_432.jpg  \n",
            "  inflating: dataset/images/img_147.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163907_067.jpg  \n",
            "  inflating: dataset/images/img_64.jpg  \n",
            "  inflating: dataset/images/img_115.jpg  \n",
            "  inflating: dataset/images/img_167.jpg  \n",
            "  inflating: dataset/images/img_577.jpg  \n",
            "  inflating: dataset/images/img_247.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115857_467.jpg  \n",
            "  inflating: dataset/images/img_239.jpg  \n",
            "  inflating: dataset/images/img_376.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114536_264.jpg  \n",
            "  inflating: dataset/images/img_564.jpg  \n",
            "  inflating: dataset/images/img_287.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120229_262.jpg  \n",
            "  inflating: dataset/images/img_16.jpg  \n",
            "  inflating: dataset/images/img_76.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115240_238.jpg  \n",
            "  inflating: dataset/images/img_178.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164252_480.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163325_790.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113121_791.jpg  \n",
            "  inflating: dataset/images/img_487.jpg  \n",
            "  inflating: dataset/images/img_426.jpg  \n",
            "  inflating: dataset/images/img_312.jpg  \n",
            "  inflating: dataset/images/img_241.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163416_906.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165224_616.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164326_966.jpg  \n",
            "  inflating: dataset/images/img_124.jpg  \n",
            "  inflating: dataset/images/img_42.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_162838_365.jpg  \n",
            "  inflating: dataset/images/img_293.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162902_156.jpg  \n",
            "  inflating: dataset/images/img_173.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120003_812.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171351_845.jpg  \n",
            "  inflating: dataset/images/img_381.jpg  \n",
            "  inflating: dataset/images/img_257.jpg  \n",
            "  inflating: dataset/images/img_330.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113328_429.jpg  \n",
            "  inflating: dataset/images/img_555.jpg  \n",
            "  inflating: dataset/images/img_328.jpg  \n",
            "  inflating: dataset/images/img_462.jpg  \n",
            "  inflating: dataset/images/img_264.jpg  \n",
            "  inflating: dataset/images/img_435.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114950_481.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163902_502.jpg  \n",
            "  inflating: dataset/images/img_56.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155730_415.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171522_187.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120258_853.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120414_055.jpg  \n",
            "  inflating: dataset/images/img_157.jpg  \n",
            "  inflating: dataset/images/img_503.jpg  \n",
            "  inflating: dataset/images/img_427.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163750_003.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115025_934.jpg  \n",
            "  inflating: dataset/images/img_268.jpg  \n",
            "  inflating: dataset/images/img_155.jpg  \n",
            "  inflating: dataset/images/img_21.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112649_789.jpg  \n",
            "  inflating: dataset/images/img_307.jpg  \n",
            "  inflating: dataset/images/img_579.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160254_127.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160223_652.jpg  \n",
            "  inflating: dataset/images/img_551.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115446_654.jpg  \n",
            "  inflating: dataset/images/img_460.jpg  \n",
            "  inflating: dataset/images/img_69.jpg  \n",
            "  inflating: dataset/images/img_32.jpg  \n",
            "  inflating: dataset/images/img_522.jpg  \n",
            "  inflating: dataset/images/img_549.jpg  \n",
            "  inflating: dataset/images/img_404.jpg  \n",
            "  inflating: dataset/images/img_411.jpg  \n",
            "  inflating: dataset/images/img_129.jpg  \n",
            "  inflating: dataset/images/img_228.jpg  \n",
            "  inflating: dataset/images/img_415.jpg  \n",
            "  inflating: dataset/images/img_227.jpg  \n",
            "  inflating: dataset/images/img_78.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164048_044.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155900_423.jpg  \n",
            "  inflating: dataset/images/img_265.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155909_715.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120311_167.jpg  \n",
            "  inflating: dataset/images/img_224.jpg  \n",
            "  inflating: dataset/images/img_93.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_170845_111.jpg  \n",
            "  inflating: dataset/images/img_322.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113653_365.jpg  \n",
            "  inflating: dataset/images/img_315.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160151_681.jpg  \n",
            "  inflating: dataset/images/img_335.jpg  \n",
            "  inflating: dataset/images/img_102.jpg  \n",
            "  inflating: dataset/images/img_177.jpg  \n",
            "  inflating: dataset/images/img_541.jpg  \n",
            "  inflating: dataset/images/img_396.jpg  \n",
            "  inflating: dataset/images/img_15.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_170904_755.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114943_844.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112046_189.jpg  \n",
            "  inflating: dataset/images/img_34.jpg  \n",
            "  inflating: dataset/images/img_192.jpg  \n",
            "  inflating: dataset/images/img_9.jpg  \n",
            "  inflating: dataset/images/img_96.jpg  \n",
            "  inflating: dataset/images/img_373.jpg  \n",
            "  inflating: dataset/images/img_445.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155852_814.jpg  \n",
            "  inflating: dataset/images/img_182.jpg  \n",
            "  inflating: dataset/images/img_400.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162524_731.jpg  \n",
            "  inflating: dataset/images/img_220.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_170812_638.jpg  \n",
            "  inflating: dataset/images/img_310.jpg  \n",
            "  inflating: dataset/images/img_19.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155703_009.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113915_692.jpg  \n",
            "  inflating: dataset/images/img_229.jpg  \n",
            "  inflating: dataset/images/img_279.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162604_516.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155752_892.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160053_731.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164447_290.jpg  \n",
            "  inflating: dataset/images/img_75.jpg  \n",
            "  inflating: dataset/images/img_500.jpg  \n",
            "  inflating: dataset/images/img_22.jpg  \n",
            "  inflating: dataset/images/img_223.jpg  \n",
            "  inflating: dataset/images/img_31.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120339_136.jpg  \n",
            "  inflating: dataset/images/img_508.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115221_464.jpg  \n",
            "  inflating: dataset/images/img_243.jpg  \n",
            "  inflating: dataset/images/img_485.jpg  \n",
            "  inflating: dataset/images/img_517.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114448_679.jpg  \n",
            "  inflating: dataset/images/img_529.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115845_517.jpg  \n",
            "  inflating: dataset/images/img_280.jpg  \n",
            "  inflating: dataset/images/img_277.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_170929_979.jpg  \n",
            "  inflating: dataset/images/img_283.jpg  \n",
            "  inflating: dataset/images/img_254.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113518_947.jpg  \n",
            "  inflating: dataset/images/img_534.jpg  \n",
            "  inflating: dataset/images/img_83.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162318_187.jpg  \n",
            "  inflating: dataset/images/img_461.jpg  \n",
            "  inflating: dataset/images/img_294.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115922_023.jpg  \n",
            "  inflating: dataset/images/img_202.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115739_494.jpg  \n",
            "  inflating: dataset/images/img_393.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171231_962.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115957_069.jpg  \n",
            "  inflating: dataset/images/img_134.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115115_297.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163513_990.jpg  \n",
            "  inflating: dataset/images/img_248.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163039_769.jpg  \n",
            "  inflating: dataset/images/img_54.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163024_700.jpg  \n",
            "  inflating: dataset/images/img_288.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112833_593.jpg  \n",
            "  inflating: dataset/images/img_451.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112731_192.jpg  \n",
            "  inflating: dataset/images/img_14.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163126_660.jpg  \n",
            "  inflating: dataset/images/img_12.jpg  \n",
            "  inflating: dataset/images/img_292.jpg  \n",
            "  inflating: dataset/images/img_465.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163525_215.jpg  \n",
            "  inflating: dataset/images/img_399.jpg  \n",
            "  inflating: dataset/images/img_30.jpg  \n",
            "  inflating: dataset/images/img_356.jpg  \n",
            "  inflating: dataset/images/img_436.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115017_100.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163038_970.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113155_401.jpg  \n",
            "  inflating: dataset/images/img_342.jpg  \n",
            "  inflating: dataset/images/img_40.jpg  \n",
            "  inflating: dataset/images/img_218.jpg  \n",
            "  inflating: dataset/images/img_359.jpg  \n",
            "  inflating: dataset/images/img_68.jpg  \n",
            "  inflating: dataset/images/img_112.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162717_525.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155634_851.jpg  \n",
            "  inflating: dataset/images/img_263.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114520_567.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113214_585.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163613_990.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163228_040.jpg  \n",
            "  inflating: dataset/images/img_18.jpg  \n",
            "  inflating: dataset/images/img_532.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164321_651.jpg  \n",
            "  inflating: dataset/images/img_430.jpg  \n",
            "  inflating: dataset/images/img_340.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171127_327.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164034_466.jpg  \n",
            "  inflating: dataset/images/img_188.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114407_028.jpg  \n",
            "  inflating: dataset/images/img_581.jpg  \n",
            "  inflating: dataset/images/img_29.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115721_057.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163542_494.jpg  \n",
            "  inflating: dataset/images/img_326.jpg  \n",
            "  inflating: dataset/images/img_67.jpg  \n",
            "  inflating: dataset/images/img_469.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114719_689.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114426_807.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163133_063.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164105_289.jpg  \n",
            "  inflating: dataset/images/img_512.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160301_990.jpg  \n",
            "  inflating: dataset/images/img_515.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113225_555.jpg  \n",
            "  inflating: dataset/images/img_519.jpg  \n",
            "  inflating: dataset/images/img_527.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155618_523.jpg  \n",
            "  inflating: dataset/images/img_3.jpg  \n",
            "  inflating: dataset/images/img_123.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112423_899.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155546_653.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115709_975.jpg  \n",
            "  inflating: dataset/images/img_267.jpg  \n",
            "  inflating: dataset/images/img_409.jpg  \n",
            "  inflating: dataset/images/img_341.jpg  \n",
            "  inflating: dataset/images/img_5.jpg  \n",
            "  inflating: dataset/images/img_190.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113628_527.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163437_066.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171154_694.jpg  \n",
            "  inflating: dataset/images/img_63.jpg  \n",
            "  inflating: dataset/images/img_94.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160106_110.jpg  \n",
            "  inflating: dataset/images/img_468.jpg  \n",
            "  inflating: dataset/images/img_33.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163421_244.jpg  \n",
            "  inflating: dataset/images/img_539.jpg  \n",
            "  inflating: dataset/images/img_57.jpg  \n",
            "  inflating: dataset/images/img_162.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114601_792.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112940_133.jpg  \n",
            "  inflating: dataset/images/img_26.jpg  \n",
            "  inflating: dataset/images/img_550.jpg  \n",
            "  inflating: dataset/images/img_203.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120104_153.jpg  \n",
            "  inflating: dataset/images/img_286.jpg  \n",
            "  inflating: dataset/images/img_303.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113946_866.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114513_321.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165102_493.jpg  \n",
            "  inflating: dataset/images/img_357.jpg  \n",
            "  inflating: dataset/images/img_333.jpg  \n",
            "  inflating: dataset/images/img_474.jpg  \n",
            "  inflating: dataset/images/img_169.jpg  \n",
            "  inflating: dataset/images/img_458.jpg  \n",
            "  inflating: dataset/images/img_366.jpg  \n",
            "  inflating: dataset/images/img_251.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163215_893.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115051_430.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112523_823.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155319_880.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120235_739.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115432_287.jpg  \n",
            "  inflating: dataset/images/img_171.jpg  \n",
            "  inflating: dataset/images/img_344.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113831_856.jpg  \n",
            "  inflating: dataset/images/img_368.jpg  \n",
            "  inflating: dataset/images/img_191.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113234_457.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114358_255.jpg  \n",
            "  inflating: dataset/images/img_494.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114744_074.jpg  \n",
            "  inflating: dataset/images/img_210.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112542_157.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115638_823.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155646_667.jpg  \n",
            "  inflating: dataset/images/img_138.jpg  \n",
            "  inflating: dataset/images/img_431.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113925_841.jpg  \n",
            "  inflating: dataset/images/img_45.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114659_740.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160401_525.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163943_931.jpg  \n",
            "  inflating: dataset/images/img_59.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171244_199.jpg  \n",
            "  inflating: dataset/images/img_81.jpg  \n",
            "  inflating: dataset/images/img_323.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112637_417.jpg  \n",
            "  inflating: dataset/images/img_152.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163117_738.jpg  \n",
            "  inflating: dataset/images/img_262.jpg  \n",
            "  inflating: dataset/images/img_185.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120208_605.jpg  \n",
            "  inflating: dataset/images/img_354.jpg  \n",
            "  inflating: dataset/images/img_377.jpg  \n",
            "  inflating: dataset/images/img_172.jpg  \n",
            "  inflating: dataset/images/img_573.jpg  \n",
            "  inflating: dataset/images/img_566.jpg  \n",
            "  inflating: dataset/images/img_246.jpg  \n",
            "  inflating: dataset/images/img_380.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163348_505.jpg  \n",
            "  inflating: dataset/images/img_433.jpg  \n",
            "  inflating: dataset/images/img_443.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162841_734.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114331_064.jpg  \n",
            "  inflating: dataset/images/img_221.jpg  \n",
            "  inflating: dataset/images/img_295.jpg  \n",
            "  inflating: dataset/images/img_308.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114711_617.jpg  \n",
            "  inflating: dataset/images/img_498.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113431_836.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114812_533.jpg  \n",
            "  inflating: dataset/images/img_90.jpg  \n",
            "  inflating: dataset/images/img_39.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155929_456.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120202_482.jpg  \n",
            "  inflating: dataset/images/img_116.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165218_465.jpg  \n",
            "  inflating: dataset/images/img_126.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163643_233.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163625_373.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162711_687.jpg  \n",
            "  inflating: dataset/images/img_305.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114021_123.jpg  \n",
            "  inflating: dataset/images/img_371.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160159_619.jpg  \n",
            "  inflating: dataset/images/img_201.jpg  \n",
            "  inflating: dataset/images/img_107.jpg  \n",
            "  inflating: dataset/images/img_100.jpg  \n",
            "  inflating: dataset/images/img_151.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163135_257.jpg  \n",
            "  inflating: dataset/images/img_452.jpg  \n",
            "  inflating: dataset/images/img_513.jpg  \n",
            "  inflating: dataset/images/img_88.jpg  \n",
            "  inflating: dataset/images/img_80.jpg  \n",
            "  inflating: dataset/images/img_256.jpg  \n",
            "  inflating: dataset/images/img_235.jpg  \n",
            "  inflating: dataset/images/img_544.jpg  \n",
            "  inflating: dataset/images/img_320.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113048_392.jpg  \n",
            "  inflating: dataset/images/img_118.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_170936_119.jpg  \n",
            "  inflating: dataset/images/img_362.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164008_174.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163533_051.jpg  \n",
            "  inflating: dataset/images/img_164.jpg  \n",
            "  inflating: dataset/images/img_70.jpg  \n",
            "  inflating: dataset/images/img_166.jpg  \n",
            "  inflating: dataset/images/img_447.jpg  \n",
            "  inflating: dataset/images/img_464.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_180441_548.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112929_150.jpg  \n",
            "  inflating: dataset/images/img_165.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163500_678.jpg  \n",
            "  inflating: dataset/images/img_109.jpg  \n",
            "  inflating: dataset/images/img_304.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164059_744.jpg  \n",
            "  inflating: dataset/images/img_153.jpg  \n",
            "  inflating: dataset/images/img_372.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165200_550.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115934_251.jpg  \n",
            "  inflating: dataset/images/img_392.jpg  \n",
            "  inflating: dataset/images/img_259.jpg  \n",
            "  inflating: dataset/images/img_198.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162854_092.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164136_299.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164020_869.jpg  \n",
            "  inflating: dataset/images/img_334.jpg  \n",
            "  inflating: dataset/images/img_507.jpg  \n",
            "  inflating: dataset/images/img_511.jpg  \n",
            "  inflating: dataset/images/img_282.jpg  \n",
            "  inflating: dataset/images/img_143.jpg  \n",
            "  inflating: dataset/images/img_13.jpg  \n",
            "  inflating: dataset/images/img_82.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162333_231.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162921_017.jpg  \n",
            "  inflating: dataset/images/img_299.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163856_350.jpg  \n",
            "  inflating: dataset/images/img_1.jpg  \n",
            "  inflating: dataset/images/img_253.jpg  \n",
            "  inflating: dataset/images/img_388.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163201_609.jpg  \n",
            "  inflating: dataset/images/img_484.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163954_311.jpg  \n",
            "  inflating: dataset/images/img_242.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163341_694.jpg  \n",
            "  inflating: dataset/images/img_439.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163437_065.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163102_725.jpg  \n",
            "  inflating: dataset/images/img_352.jpg  \n",
            "  inflating: dataset/images/img_7.jpg  \n",
            "  inflating: dataset/images/img_526.jpg  \n",
            "  inflating: dataset/images/img_325.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163112_773.jpg  \n",
            "  inflating: dataset/images/img_559.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162830_170.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160050_083.jpg  \n",
            "  inflating: dataset/images/img_302.jpg  \n",
            "  inflating: dataset/images/img_156.jpg  \n",
            "  inflating: dataset/images/img_314.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162447_155.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112747_882.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_162903_421.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162704_874.jpg  \n",
            "  inflating: dataset/images/img_36.jpg  \n",
            "  inflating: dataset/images/img_71.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115209_941.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171256_778.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163303_769.jpg  \n",
            "  inflating: dataset/images/img_499.jpg  \n",
            "  inflating: dataset/images/img_347.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112816_208.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112237_905.jpg  \n",
            "  inflating: dataset/images/img_582.jpg  \n",
            "  inflating: dataset/images/img_456.jpg  \n",
            "  inflating: dataset/images/img_471.jpg  \n",
            "  inflating: dataset/images/img_111.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_180508_115.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164436_828.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113340_867.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114218_054.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162749_017.jpg  \n",
            "  inflating: dataset/images/img_274.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114834_649.jpg  \n",
            "  inflating: dataset/images/img_493.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163824_556.jpg  \n",
            "  inflating: dataset/images/img_575.jpg  \n",
            "  inflating: dataset/images/img_505.jpg  \n",
            "  inflating: dataset/images/img_504.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112741_700.jpg  \n",
            "  inflating: dataset/images/img_547.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171205_218.jpg  \n",
            "  inflating: dataset/images/img_348.jpg  \n",
            "  inflating: dataset/images/img_309.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162934_572.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113811_497.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163240_261.jpg  \n",
            "  inflating: dataset/images/img_301.jpg  \n",
            "  inflating: dataset/images/img_489.jpg  \n",
            "  inflating: dataset/images/img_186.jpg  \n",
            "  inflating: dataset/images/img_289.jpg  \n",
            "  inflating: dataset/images/img_440.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115330_058.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114208_577.jpg  \n",
            "  inflating: dataset/images/img_306.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160007_697.jpg  \n",
            "  inflating: dataset/images/img_490.jpg  \n",
            "  inflating: dataset/images/img_127.jpg  \n",
            "  inflating: dataset/images/img_108.jpg  \n",
            "  inflating: dataset/images/img_298.jpg  \n",
            "  inflating: dataset/images/img_97.jpg  \n",
            "  inflating: dataset/images/img_87.jpg  \n",
            "  inflating: dataset/images/img_576.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155845_074.jpg  \n",
            "  inflating: dataset/images/img_50.jpg  \n",
            "  inflating: dataset/images/img_482.jpg  \n",
            "  inflating: dataset/images/img_578.jpg  \n",
            "  inflating: dataset/images/img_395.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120315_338.jpg  \n",
            "  inflating: dataset/images/img_43.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155630_982.jpg  \n",
            "  inflating: dataset/images/img_62.jpg  \n",
            "  inflating: dataset/images/img_65.jpg  \n",
            "  inflating: dataset/images/img_528.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163320_209.jpg  \n",
            "  inflating: dataset/images/img_455.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114455_721.jpg  \n",
            "  inflating: dataset/images/img_113.jpg  \n",
            "  inflating: dataset/images/img_378.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163629_572.jpg  \n",
            "  inflating: dataset/images/img_4.jpg  \n",
            "  inflating: dataset/images/img_545.jpg  \n",
            "  inflating: dataset/images/img_422.jpg  \n",
            "  inflating: dataset/images/img_255.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112515_834.jpg  \n",
            "  inflating: dataset/images/img_501.jpg  \n",
            "  inflating: dataset/images/img_125.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115150_053.jpg  \n",
            "  inflating: dataset/images/img_316.jpg  \n",
            "  inflating: dataset/images/img_491.jpg  \n",
            "  inflating: dataset/images/img_463.jpg  \n",
            "  inflating: dataset/images/img_74.jpg  \n",
            "  inflating: dataset/images/img_488.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163101_195.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115751_086.jpg  \n",
            "  inflating: dataset/images/img_163.jpg  \n",
            "  inflating: dataset/images/img_327.jpg  \n",
            "  inflating: dataset/images/img_133.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120123_343.jpg  \n",
            "  inflating: dataset/images/img_197.jpg  \n",
            "  inflating: dataset/images/img_252.jpg  \n",
            "  inflating: dataset/images/img_101.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113548_881.jpg  \n",
            "  inflating: dataset/images/img_444.jpg  \n",
            "  inflating: dataset/images/img_403.jpg  \n",
            "  inflating: dataset/images/img_360.jpg  \n",
            "  inflating: dataset/images/img_160.jpg  \n",
            "  inflating: dataset/images/img_492.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163014_903.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114416_148.jpg  \n",
            "  inflating: dataset/images/img_437.jpg  \n",
            "  inflating: dataset/images/img_556.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112710_664.jpg  \n",
            "  inflating: dataset/images/img_459.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112324_536.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114632_872.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171248_876.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155955_058.jpg  \n",
            "  inflating: dataset/images/img_131.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114337_808.jpg  \n",
            "  inflating: dataset/images/img_441.jpg  \n",
            "  inflating: dataset/images/img_196.jpg  \n",
            "  inflating: dataset/images/img_219.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164405_113.jpg  \n",
            "  inflating: dataset/images/img_557.jpg  \n",
            "  inflating: dataset/images/img_345.jpg  \n",
            "  inflating: dataset/images/img_168.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114311_486.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171450_426.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163033_103.jpg  \n",
            "  inflating: dataset/images/img_442.jpg  \n",
            "  inflating: dataset/images/img_215.jpg  \n",
            "  inflating: dataset/images/img_343.jpg  \n",
            "  inflating: dataset/images/img_28.jpg  \n",
            "  inflating: dataset/images/img_276.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113751_729.jpg  \n",
            "  inflating: dataset/images/img_311.jpg  \n",
            "  inflating: dataset/images/img_387.jpg  \n",
            "  inflating: dataset/images/img_233.jpg  \n",
            "  inflating: dataset/images/img_367.jpg  \n",
            "  inflating: dataset/images/img_149.jpg  \n",
            "  inflating: dataset/images/img_121.jpg  \n",
            "  inflating: dataset/images/img_536.jpg  \n",
            "  inflating: dataset/images/img_318.jpg  \n",
            "  inflating: dataset/images/img_369.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113140_437.jpg  \n",
            "  inflating: dataset/images/img_391.jpg  \n",
            "  inflating: dataset/images/img_132.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162420_169.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171423_973.jpg  \n",
            "  inflating: dataset/images/img_281.jpg  \n",
            "  inflating: dataset/images/img_394.jpg  \n",
            "  inflating: dataset/images/img_419.jpg  \n",
            "  inflating: dataset/images/img_245.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155831_628.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155451_921.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165255_275.jpg  \n",
            "  inflating: dataset/images/img_554.jpg  \n",
            "  inflating: dataset/images/img_2.jpg  \n",
            "  inflating: dataset/images/img_174.jpg  \n",
            "  inflating: dataset/images/img_205.jpg  \n",
            "  inflating: dataset/images/img_47.jpg  \n",
            "  inflating: dataset/images/img_552.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163356_066.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115104_122.jpg  \n",
            "  inflating: dataset/images/img_199.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120020_533.jpg  \n",
            "  inflating: dataset/images/img_49.jpg  \n",
            "  inflating: dataset/images/img_434.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163640_425.jpg  \n",
            "  inflating: dataset/images/img_8.jpg  \n",
            "  inflating: dataset/images/img_336.jpg  \n",
            "  inflating: dataset/images/img_329.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113311_981.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163153_892.jpg  \n",
            "  inflating: dataset/images/img_454.jpg  \n",
            "  inflating: dataset/images/img_158.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165338_827.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155357_827.jpg  \n",
            "  inflating: dataset/images/img_110.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114302_066.jpg  \n",
            "  inflating: dataset/images/img_208.jpg  \n",
            "  inflating: dataset/images/img_51.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113416_297.jpg  \n",
            "  inflating: dataset/images/img_181.jpg  \n",
            "  inflating: dataset/images/img_390.jpg  \n",
            "  inflating: dataset/images/img_77.jpg  \n",
            "  inflating: dataset/images/img_405.jpg  \n",
            "  inflating: dataset/images/img_408.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114345_068.jpg  \n",
            "  inflating: dataset/images/img_382.jpg  \n",
            "  inflating: dataset/images/img_148.jpg  \n",
            "  inflating: dataset/images/img_421.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163601_589.jpg  \n",
            "  inflating: dataset/images/img_105.jpg  \n",
            "  inflating: dataset/images/img_560.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162541_643.jpg  \n",
            "  inflating: dataset/images/img_384.jpg  \n",
            "  inflating: dataset/images/img_446.jpg  \n",
            "  inflating: dataset/images/img_449.jpg  \n",
            "  inflating: dataset/images/img_375.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112617_765.jpg  \n",
            "  inflating: dataset/images/img_402.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113409_817.jpg  \n",
            "  inflating: dataset/images/img_25.jpg  \n",
            "  inflating: dataset/images/img_261.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163021_872.jpg  \n",
            "  inflating: dataset/images/img_317.jpg  \n",
            "  inflating: dataset/images/img_574.jpg  \n",
            "  inflating: dataset/images/img_284.jpg  \n",
            "  inflating: dataset/images/img_92.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120111_559.jpg  \n",
            "  inflating: dataset/images/img_41.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115821_574.jpg  \n",
            "  inflating: dataset/images/img_346.jpg  \n",
            "  inflating: dataset/images/img_114.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113802_686.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163025_525.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120428_063.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155923_140.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115728_788.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164013_481.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113206_606.jpg  \n",
            "  inflating: dataset/images/img_398.jpg  \n",
            "  inflating: dataset/images/img_145.jpg  \n",
            "  inflating: dataset/images/img_313.jpg  \n",
            "  inflating: dataset/images/img_179.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112247_482.jpg  \n",
            "  inflating: dataset/images/img_457.jpg  \n",
            "  inflating: dataset/images/img_535.jpg  \n",
            "  inflating: dataset/images/img_567.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_162844_746.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171334_993.jpg  \n",
            "  inflating: dataset/images/img_521.jpg  \n",
            "  inflating: dataset/images/img_420.jpg  \n",
            "  inflating: dataset/images/img_35.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160321_061.jpg  \n",
            "  inflating: dataset/images/img_416.jpg  \n",
            "  inflating: dataset/images/img_244.jpg  \n",
            "  inflating: dataset/images/img_260.jpg  \n",
            "  inflating: dataset/images/img_385.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114725_361.jpg  \n",
            "  inflating: dataset/images/img_213.jpg  \n",
            "  inflating: dataset/images/img_99.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112006_087.jpg  \n",
            "  inflating: dataset/images/img_530.jpg  \n",
            "  inflating: dataset/images/img_379.jpg  \n",
            "  inflating: dataset/images/img_46.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163917_811.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_180426_862.jpg  \n",
            "  inflating: dataset/images/img_319.jpg  \n",
            "  inflating: dataset/images/img_184.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112447_575.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155431_087.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115758_769.jpg  \n",
            "  inflating: dataset/images/img_353.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155159_641.jpg  \n",
            "  inflating: dataset/images/img_217.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163519_335.jpg  \n",
            "  inflating: dataset/images/img_195.jpg  \n",
            "  inflating: dataset/images/img_84.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163143_879.jpg  \n",
            "  inflating: dataset/images/img_510.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160024_605.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120029_011.jpg  \n",
            "  inflating: dataset/images/img_271.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114937_416.jpg  \n",
            "  inflating: dataset/images/img_506.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112152_523.jpg  \n",
            "  inflating: dataset/images/img_204.jpg  \n",
            "  inflating: dataset/images/img_225.jpg  \n",
            "  inflating: dataset/images/img_154.jpg  \n",
            "  inflating: dataset/images/img_438.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165134_757.jpg  \n",
            "  inflating: dataset/images/img_472.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112702_487.jpg  \n",
            "  inflating: dataset/images/img_429.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163252_808.jpg  \n",
            "  inflating: dataset/images/img_222.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115407_015.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_160241_691.jpg  \n",
            "  inflating: dataset/images/img_486.jpg  \n",
            "  inflating: dataset/images/img_37.jpg  \n",
            "  inflating: dataset/images/img_291.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114737_157.jpg  \n",
            "  inflating: dataset/images/img_324.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115426_588.jpg  \n",
            "  inflating: dataset/images/img_297.jpg  \n",
            "  inflating: dataset/images/img_417.jpg  \n",
            "  inflating: dataset/images/img_278.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_112901_410.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114918_049.jpg  \n",
            "  inflating: dataset/images/img_374.jpg  \n",
            "  inflating: dataset/images/img_553.jpg  \n",
            "  inflating: dataset/images/img_128.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113041_190.jpg  \n",
            "  inflating: dataset/images/img_349.jpg  \n",
            "  inflating: dataset/images/img_10.jpg  \n",
            "  inflating: dataset/images/img_187.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114051_748.jpg  \n",
            "  inflating: dataset/images/img_120.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164146_143.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_155723_947.jpg  \n",
            "  inflating: dataset/images/img_432.jpg  \n",
            "  inflating: dataset/images/img_60.jpg  \n",
            "  inflating: dataset/images/img_568.jpg  \n",
            "  inflating: dataset/images/img_52.jpg  \n",
            "  inflating: dataset/images/img_135.jpg  \n",
            "  inflating: dataset/images/img_518.jpg  \n",
            "  inflating: dataset/images/img_23.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163447_798.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113305_150.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171510_740.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164154_779.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_154808_599.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113059_495.jpg  \n",
            "  inflating: dataset/images/img_497.jpg  \n",
            "  inflating: dataset/images/img_216.jpg  \n",
            "  inflating: dataset/images/img_563.jpg  \n",
            "  inflating: dataset/images/img_548.jpg  \n",
            "  inflating: dataset/images/img_137.jpg  \n",
            "  inflating: dataset/images/img_331.jpg  \n",
            "  inflating: dataset/images/img_290.jpg  \n",
            "  inflating: dataset/images/img_355.jpg  \n",
            "  inflating: dataset/images/img_79.jpg  \n",
            "  inflating: dataset/images/img_516.jpg  \n",
            "  inflating: dataset/images/img_358.jpg  \n",
            "  inflating: dataset/images/img_207.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113253_814.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113529_564.jpg  \n",
            "  inflating: dataset/images/img_270.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114911_188.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_163511_016.jpg  \n",
            "  inflating: dataset/images/img_538.jpg  \n",
            "  inflating: dataset/images/img_130.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120349_098.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162800_407.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114730_194.jpg  \n",
            "  inflating: dataset/images/img_558.jpg  \n",
            "  inflating: dataset/images/img_296.jpg  \n",
            "  inflating: dataset/images/img_365.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113023_779.jpg  \n",
            "  inflating: dataset/images/img_250.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115804_336.jpg  \n",
            "  inflating: dataset/images/img_258.jpg  \n",
            "  inflating: dataset/images/img_89.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162413_074.jpg  \n",
            "  inflating: dataset/images/img_571.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114506_234.jpg  \n",
            "  inflating: dataset/images/img_414.jpg  \n",
            "  inflating: dataset/images/img_200.jpg  \n",
            "  inflating: dataset/images/img_240.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114626_944.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120306_583.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_171435_163.jpg  \n",
            "  inflating: dataset/images/img_428.jpg  \n",
            "  inflating: dataset/images/img_209.jpg  \n",
            "  inflating: dataset/images/img_565.jpg  \n",
            "  inflating: dataset/images/img_266.jpg  \n",
            "  inflating: dataset/images/img_232.jpg  \n",
            "  inflating: dataset/images/img_86.jpg  \n",
            "  inflating: dataset/images/img_285.jpg  \n",
            "  inflating: dataset/images/img_407.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120241_136.jpg  \n",
            "  inflating: dataset/images/img_231.jpg  \n",
            "  inflating: dataset/images/img_269.jpg  \n",
            "  inflating: dataset/images/img_477.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_113244_655.jpg  \n",
            "  inflating: dataset/images/img_397.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163407_260.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114647_065.jpg  \n",
            "  inflating: dataset/images/img_146.jpg  \n",
            "  inflating: dataset/images/img_58.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_162726_054.jpg  \n",
            "  inflating: dataset/images/img_91.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_120212_122.jpg  \n",
            "  inflating: dataset/images/img_142.jpg  \n",
            "  inflating: dataset/images/img_6.jpg  \n",
            "  inflating: dataset/images/img_483.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_115312_920.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165145_728.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_164127_773.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_165526_567.jpg  \n",
            "  inflating: dataset/images/img_237.jpg  \n",
            "  inflating: dataset/images/img_572.jpg  \n",
            "  inflating: dataset/images/IMG_20200205_114544_065.jpg  \n",
            "  inflating: dataset/images/img_540.jpg  \n",
            "  inflating: dataset/images/img_546.jpg  \n",
            "  inflating: dataset/images/img_370.jpg  \n",
            "  inflating: dataset/images/img_236.jpg  \n",
            "  inflating: dataset/images/IMG_20200204_163847_629.jpg  \n",
            "  inflating: dataset/images/img_495.jpg  \n",
            "  inflating: dataset/images/img_161.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZAB27pkQmBl"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYom3RyicH9K"
      },
      "source": [
        "Import modules for data preparation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKZeUABicHCr"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import shutil\n",
        "import csv\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQTUkOXha0CX"
      },
      "source": [
        "# Folder containing training images.\n",
        "IMAGES_DIR = r'/content/object_detection/dataset/images' # Directory containing training images\n",
        "LABELS_PATH = r'/content/object_detection/dataset/labels.json' # Path to the JSON labels file"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOqCnwMIeuEE"
      },
      "source": [
        "### Utility Functions\n",
        "To understand the bounding boxes and what each value in the bbox list represents, we draw from the COCO labeling format.\n",
        "\n",
        "---\n",
        "\n",
        "Labelbox creates a bounding box as follows:\n",
        "```\n",
        "bbox = ['xmin', 'ymin', 'width', 'height']\n",
        "```\n",
        "**`xmin` and `ymin` refer to the top left axis of the image.**\n",
        "\n",
        "---\n",
        "\n",
        "Further reading:\n",
        "1. [https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXyhCRMOmY9m"
      },
      "source": [
        "def json_to_csv_and_pbtxt(images_dir, labels_path, annotations_dir):\n",
        "  \"\"\"\n",
        "  Converts a JSON file to a csv file and pbtxt file.\n",
        "\n",
        "  TensorFlow requires a label map, which maps each of the used labels \n",
        "  to an integer values. This label map is used both by the training and detection \n",
        "  processes. Notice the labels are one-indexed i.e. start at 1 (one).\n",
        "\n",
        "  Example:\n",
        "  # example.pbtxt\n",
        "  item {\n",
        "    id: 1\n",
        "    name: 'cat'\n",
        "  }\n",
        "\n",
        "  item {\n",
        "    id: 2\n",
        "    name: 'dog'\n",
        "  }\n",
        "\n",
        "  Both files are stored under the annotations folder.\n",
        "\n",
        "  Note: This function is specific to labelbox labeling format.\n",
        "\n",
        "  images_dir       is the directory containing the training images.\n",
        "  labels_path      is the path to the labels JSON file.\n",
        "  annotations_dir  is the directory in which the annotations will be stored.\n",
        "  \"\"\"\n",
        "  # Stores the data from the JSON file.\n",
        "  data_list = []\n",
        "\n",
        "  # Open the labels.json from labels_dir\n",
        "  with open(labels_path) as labels: \n",
        "        data_list.extend(json.load(labels)) \n",
        "      \n",
        "  # Generating the csv in the annotations folder under data directory. (Ideally)\n",
        "  csv_file_name = os.path.join(str(annotations_dir), 'labels.csv')\n",
        "\n",
        "  with open(csv_file_name, 'w') as csv_label_file:\n",
        "    f = csv.writer(csv_label_file)\n",
        "    f.writerow(['file_name', 'width', 'height', 'class', 'xmin', 'ymin', \n",
        "                'xmax', 'ymax'])\n",
        "\n",
        "    for item in tqdm(data_list, desc = \"Images\"):\n",
        "        item_label = item['Label']\n",
        "\n",
        "        # Check if the item['Label'] is empty. If empty, continue to the next item.\n",
        "        if bool(item_label) == False:\n",
        "            continue\n",
        "        # Check if the item['Label']['objects'] is empty, if so, proceed to the next item.\n",
        "        elif bool(item_label['objects'] == False):\n",
        "            continue\n",
        "        \n",
        "        # Image file_name\n",
        "        file_name = item['External ID']\n",
        "\n",
        "        for bounding_box in item_label['objects']:\n",
        "          class_name = bounding_box['title'].replace(\" \", \"\")\n",
        "\n",
        "          top = bounding_box['bbox']['top'] \n",
        "          left = bounding_box['bbox']['left']\n",
        "          height = bounding_box['bbox']['height'] \n",
        "          width = bounding_box['bbox']['width']\n",
        "                \n",
        "          # Note that the axis refers to the top left corner of the image\n",
        "          xmin = left \n",
        "          xmax = xmin + width\n",
        "          ymin = top \n",
        "          ymax = ymin + height\n",
        "          f.writerow([file_name, width, height, class_name, xmin, ymin, xmax, ymax])\n",
        "                \n",
        "  # Create the label map\n",
        "  label_map_path = os.path.join(annotations_dir, \"label_map.pbtxt\")\n",
        "  pbtxt_content = \"\"\n",
        "\n",
        "  pbtxt_content = (\n",
        "      pbtxt_content\n",
        "      + \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\\n\".format(1, class_name)\n",
        "  )\n",
        "  pbtxt_content = pbtxt_content.strip()\n",
        "  with open(label_map_path, \"w\") as f:\n",
        "      f.write(pbtxt_content)              "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwqCNIW02O9T"
      },
      "source": [
        "# TODO\n",
        "# 1. Create the respective folders as defined above for the training directory.\n",
        "# 2. Create the labels.csv and the labels_map.pbtxt files and store in the respective folders\n",
        "#    using the function defined above.\n",
        "\n",
        "# Note: Take care to use the correct directory and file paths.  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe_Ip0I_BQD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53b9096-c7dd-4b33-d256-41edc09a6dc6"
      },
      "source": [
        "json_to_csv_and_pbtxt(IMAGES_DIR, LABELS_PATH, r'/content/object_detection/workspace/data/annotations')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Images: 100%|██████████| 581/581 [00:00<00:00, 117011.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_ex_xg_keqv"
      },
      "source": [
        "The `annotations` folder should now contain two files: `labels.csv' and 'label_map.pbtxt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZJB13wZ9tAh"
      },
      "source": [
        "Split the `labels.csv` (remember it contains the entire training data) into train (0.8) and test set (0.2). For your expirements, create a validation set from the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOo5067-9TS6"
      },
      "source": [
        "# The pandas library is used.\n",
        "import pandas as pd"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtBkPHBoE0jU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "62bcb279-5c57-44f6-bc87-fa3c4fa70cdf"
      },
      "source": [
        "# NB: Ensure to set the ANNOTATIONS_DIR to the annotations directory.\n",
        "dataset = pd.read_csv(os.path.join(r'/content/object_detection/workspace/data/annotations','labels.csv'))\n",
        "dataset.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>img_17.jpg</td>\n",
              "      <td>54</td>\n",
              "      <td>84</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>223</td>\n",
              "      <td>10</td>\n",
              "      <td>277</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>img_17.jpg</td>\n",
              "      <td>22</td>\n",
              "      <td>26</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>369</td>\n",
              "      <td>312</td>\n",
              "      <td>391</td>\n",
              "      <td>338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>img_562.jpg</td>\n",
              "      <td>56</td>\n",
              "      <td>70</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>80</td>\n",
              "      <td>70</td>\n",
              "      <td>136</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>img_530.jpg</td>\n",
              "      <td>29</td>\n",
              "      <td>27</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>130</td>\n",
              "      <td>275</td>\n",
              "      <td>159</td>\n",
              "      <td>302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>img_498.jpg</td>\n",
              "      <td>67</td>\n",
              "      <td>37</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>221</td>\n",
              "      <td>13</td>\n",
              "      <td>288</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     file_name  width  height      class  xmin  ymin  xmax  ymax\n",
              "0   img_17.jpg     54      84  brownspot   223    10   277    94\n",
              "1   img_17.jpg     22      26  brownspot   369   312   391   338\n",
              "2  img_562.jpg     56      70  brownspot    80    70   136   140\n",
              "3  img_530.jpg     29      27  brownspot   130   275   159   302\n",
              "4  img_498.jpg     67      37  brownspot   221    13   288    50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71SvN6sb_NM-"
      },
      "source": [
        "# Spliting the dataset into train (0.8) and test (0.2) sets respectively.\n",
        "dataset_copy = dataset.copy()\n",
        "train_set = dataset_copy.sample(frac=0.8, random_state=42)\n",
        "test_set = dataset_copy.drop(train_set.index)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PHMjbOsAmLP"
      },
      "source": [
        "There are a couple of methods that can be used to split the dataset into train and test sets. A popular option is to use the  [sklearn train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9WoJuHW_-11"
      },
      "source": [
        "# Write the test_set and train_set to file.\n",
        "train_set.to_csv(os.path.join(r'/content/object_detection/workspace/data/annotations', 'train_labels.csv'), index=False)\n",
        "test_set.to_csv(os.path.join(r'/content/object_detection/workspace/data/annotations', 'test_labels.csv'), index=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_RnayqpCxEu"
      },
      "source": [
        "Using the `train_set` and `test_set` we shall move the files to their respective folders in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn8TdiOxB9e-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a2fc7121-d53e-4fcf-a6dc-abb7241170fc"
      },
      "source": [
        "train_set.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>801</th>\n",
              "      <td>img_125.jpg</td>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>96</td>\n",
              "      <td>352</td>\n",
              "      <td>109</td>\n",
              "      <td>370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>677</th>\n",
              "      <td>img_447.jpg</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>81</td>\n",
              "      <td>113</td>\n",
              "      <td>123</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>img_174.jpg</td>\n",
              "      <td>39</td>\n",
              "      <td>36</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>149</td>\n",
              "      <td>241</td>\n",
              "      <td>188</td>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>img_420.jpg</td>\n",
              "      <td>102</td>\n",
              "      <td>139</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>256</td>\n",
              "      <td>0</td>\n",
              "      <td>358</td>\n",
              "      <td>139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>img_363.jpg</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>brownspot</td>\n",
              "      <td>111</td>\n",
              "      <td>98</td>\n",
              "      <td>132</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       file_name  width  height      class  xmin  ymin  xmax  ymax\n",
              "801  img_125.jpg     13      18  brownspot    96   352   109   370\n",
              "677  img_447.jpg     42      42  brownspot    81   113   123   155\n",
              "43   img_174.jpg     39      36  brownspot   149   241   188   277\n",
              "990  img_420.jpg    102     139  brownspot   256     0   358   139\n",
              "70   img_363.jpg     21      22  brownspot   111    98   132   120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wu2rM9jCudS"
      },
      "source": [
        "# Move the train files to the train folder under the data directory.\n",
        "# We use the copyfile function from the shutil library.\n",
        "from shutil import copyfile"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2nD1KzIDnc_"
      },
      "source": [
        "# Make sure the respective directories exist and or \n",
        "# keep track of your current directory.\n",
        "# os.mkdirs(directory)\n",
        "\n",
        "def move_files(files, source, dest):\n",
        "  \"\"\"Move files from the source directory to the destination directory.\"\"\"\n",
        "  for filename in files:\n",
        "    copyfile(os.path.join(source, filename),\n",
        "                 os.path.join(dest, filename))\n",
        "\n",
        "TRAIN_DIR = r'workspace/data/images/train'\n",
        "TEST_DIR = r'workspace/data/images/test'\n",
        "\n",
        "TRAIN_IMAGES = train_set['file_name'].tolist()\n",
        "TEST_IMAGES = test_set['file_name'].tolist()\n",
        "\n",
        "# Move train files\n",
        "move_files(TRAIN_IMAGES, IMAGES_DIR, TRAIN_DIR)\n",
        "# Move test files\n",
        "move_files(TEST_IMAGES, IMAGES_DIR, TEST_DIR)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWojJtdaKLbZ"
      },
      "source": [
        "Confirm that the images have been moved to the respective folders and clear the original folder to free up disk space.\n",
        "\n",
        "\n",
        "```\n",
        "# !rm -rf pathto/original_dataset\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ-B0KuEL9rS"
      },
      "source": [
        "## 2. Generate TFRecords from the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x4Ozo2bJ9zy"
      },
      "source": [
        "# Support imports\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.append(\"../models/research\")\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03hu0-X0O27u"
      },
      "source": [
        "def class_text_to_int(row_label):\n",
        "  if row_label == 'brown_spot': # the respective class_name\n",
        "    return 1\n",
        "  else:\n",
        "    None\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['file_name', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    # check if the image format is matching with your images.\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/file_name': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOavDKnGlM0-"
      },
      "source": [
        "# TODO\n",
        "# 1. Create TFRecords for both train.csv and test.csv.\n",
        "\n",
        "labels = ['train_labels', 'test_labels']\n",
        "paths = [TRAIN_DIR, TEST_DIR]\n",
        "\n",
        "# Implement the creation of the *.record files using the functions defined above."
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS-iTSwMRbc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "e415169d-fc1d-49f0-eb09-b01ecef81914"
      },
      "source": [
        "# Create tfrecords for train and test\n",
        "for csv in ['train_labels']:\n",
        "  writer = tf.io.TFRecordWriter('/content/object_detection/workspace/data/images/train/' + csv + '.record')\n",
        "  path = r'/content/object_detection/dataset/images'\n",
        "  examples = pd.read_csv( '/content/object_detection/workspace/data/annotations/train_labels.csv')\n",
        "  grouped = split(examples, 'file_name')\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "    \n",
        "  writer.close()\n",
        "  print('Successfully created the TFRecords: {}'.format('/content/object_detection/workspace/data/images/train/' +csv + '.record'))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-f5d1e3840b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'file_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtf_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tf_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-3c060deabcbc>\u001b[0m in \u001b[0;36mcreate_tf_example\u001b[0;34m(group, path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_tf_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mencoded_jpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mencoded_jpg_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_jpg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'data' object has no attribute 'filename'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfPLwxY8k6X7"
      },
      "source": [
        "The `annotations` folder should now contain two more files: `test_labels.record` and `train_labels.record`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjs9KCzpWznq"
      },
      "source": [
        "## Configure a simple training pipeline.\n",
        "We use pretrained weights (randomly choosen from the [TensorFlow’s detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)). For this tutorial, we shall use [faster_rcnn_inception_v2_coco_2018_01_28](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz).\n",
        "\n",
        "---\n",
        "\n",
        "Steps to follow:\n",
        "1. Download the pretrained weights of your choice from [TensorFlow’s detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
        "2. Download the corresponding config file from [here](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs) and save it under the `data/training` folder.\n",
        "3. Extract the `*.tar.gz` file into the `pre-trained-model` folder under the `data` directory. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc0rSqxLehWM"
      },
      "source": [
        "# Helper library\n",
        "import urllib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH-tkF9gTGOm"
      },
      "source": [
        "# 1. Download the pretrained weights of your choice from TensorFlow’s detection model zoo.\n",
        "PRE_TRAINED_MODEL_DIR = r'workspace/data/pre-trained-model'\n",
        "MODEL = 'faster_rcnn_inception_v2_coco_2018_01_28.tar.gz'\n",
        "\n",
        "if MODEL not in os.listdir(PRE_TRAINED_MODEL_DIR):\n",
        "  urllib.request.urlretrieve(f'http://download.tensorflow.org/models/object_detection/{MODEL}',\\\n",
        "                     os.path.join(PRE_TRAINED_MODEL_DIR, MODEL))\n",
        "  !tar -zxvf {PRE_TRAINED_MODEL_DIR}/{MODEL} -C {PRE_TRAINED_MODEL_DIR} # Extract to a given directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBmkF90GfaYJ"
      },
      "source": [
        "# 2. Download the corresponding config file from here and save it under the data/training folder.\n",
        "# config_file = 'faster_rcnn_inception_v2_coco.config'\n",
        "# TRAINING_DIR = r'object_detection/workspace/data/training'\n",
        "\n",
        "# if config_file not in os.listdir(TRAINING_DIR):\n",
        "#   urllib.request.urlretrieve(f'https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/{config_file}', \\\n",
        "#                              os.path.join(TRAINING_DIR, config_file))\n",
        "\n",
        "# TODO: The code downloads an HTML file instead of the .config, requires further exploration."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m-u_1JzoG7Z"
      },
      "source": [
        "Now open and edit the config file to fit your dataset. The config file also contains numerous hyperparameters that will be continously optimized for your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRNiWZgEg8Me"
      },
      "source": [
        "# This is the .config file for faster_rcnn_incpetion_v2_coco.config, copied from\n",
        "# https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs\n",
        "\n",
        "%%writefile /content/object_detection/workspace/data/training/faster_rcnn_inception_v2_coco.config\n",
        "\n",
        "# Open the file to see it's contents.\n",
        "# Faster R-CNN with Inception v2, configuration for MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "\n",
        "model {\n",
        "  faster_rcnn {\n",
        "    num_classes: 1 # Change to the number of classes in your dataset.\n",
        "    image_resizer {\n",
        "      keep_aspect_ratio_resizer {\n",
        "        min_dimension: 400\n",
        "        max_dimension: 1024\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'faster_rcnn_inception_v2' # Change to the pretrained weights that you are using.\n",
        "      first_stage_features_stride: 16\n",
        "    }\n",
        "    first_stage_anchor_generator {\n",
        "      grid_anchor_generator {\n",
        "        scales: [0.25, 0.5, 1.0, 2.0]\n",
        "        aspect_ratios: [0.5, 1.0, 2.0]\n",
        "        height_stride: 16\n",
        "        width_stride: 16\n",
        "      }\n",
        "    }\n",
        "    first_stage_box_predictor_conv_hyperparams {\n",
        "      op: CONV\n",
        "      regularizer {\n",
        "        l2_regularizer {\n",
        "          weight: 0.0\n",
        "        }\n",
        "      }\n",
        "      initializer {\n",
        "        truncated_normal_initializer {\n",
        "          stddev: 0.01\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    first_stage_nms_score_threshold: 0.0\n",
        "    first_stage_nms_iou_threshold: 0.7\n",
        "    first_stage_max_proposals: 300\n",
        "    first_stage_localization_loss_weight: 2.0\n",
        "    first_stage_objectness_loss_weight: 1.0\n",
        "    initial_crop_size: 14\n",
        "    maxpool_kernel_size: 2\n",
        "    maxpool_stride: 2\n",
        "    second_stage_box_predictor {\n",
        "      mask_rcnn_box_predictor {\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 1.0\n",
        "        fc_hyperparams {\n",
        "          op: FC\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.0\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            variance_scaling_initializer {\n",
        "              factor: 1.0\n",
        "              uniform: true\n",
        "              mode: FAN_AVG\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    second_stage_post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 0.0\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 300\n",
        "      }\n",
        "      score_converter: SOFTMAX\n",
        "    }\n",
        "    second_stage_localization_loss_weight: 2.0\n",
        "    second_stage_classification_loss_weight: 1.0\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 4 # Increase/Decrease considering available memory.\n",
        "  optimizer {\n",
        "    momentum_optimizer: {\n",
        "      learning_rate: {\n",
        "        manual_step_learning_rate {\n",
        "          initial_learning_rate: 0.0002\n",
        "          schedule {\n",
        "            step: 900000\n",
        "            learning_rate: .00002\n",
        "          }\n",
        "          schedule {\n",
        "            step: 1200000\n",
        "            learning_rate: .000002\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "    }\n",
        "    use_moving_average: false\n",
        "  }\n",
        "  gradient_clipping_by_norm: 10.0\n",
        "  fine_tune_checkpoint: \"/content/object_detection/workspace/data/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt\" # Replace with your pretrained weights.\n",
        "  from_detection_checkpoint: true\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the COCO dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/object_detection/workspace/data/annotations/train_labels.record\" # Path to training TFRecord\n",
        "  }\n",
        "  label_map_path: \"/content/object_detection/workspace/data/annotations/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  # num_examples: 8000\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  # max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/object_detection/workspace/data/annotations/test_labels.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/object_detection/workspace/data/annotations/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNI-JGMMZbyJ"
      },
      "source": [
        "## Monitoring training progress.\n",
        "Tensorflow provides the [Tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) utility library that allows you to continuously monitor and visualize a number of different training/evaluation metrics. \n",
        "\n",
        "---\n",
        "If you run Tensorboard before training the model, then you are able to monitor progress concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU5M7fWFbjik"
      },
      "source": [
        "# 1. Run the command below to run tensorboard, --logdir should point to the folder that stores the \n",
        "#    checkpoints during training.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=workspace/data/training/ --port=6006"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVTiRDdCwPJ3"
      },
      "source": [
        "## Training a model\n",
        "For training, copy the `object_detection/models/research/object_detection/model_main.py` file to `object_detection/workspace/data` folder.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5SC4UlOqkLD"
      },
      "source": [
        "# 1. Copy the model_main.py into the object_detection/workspace/data\n",
        "copyfile('models/research/object_detection/model_main_tf2.py',\\\n",
        "         'workspace/data/model_main_tf2.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEXfWmL8VFdV"
      },
      "source": [
        "# 3. Run model_main.py script to start training, take note of the different arguments\n",
        "#    supplied to the script and update accordingly.\n",
        "!python workspace/data/model_main_tf2.py \\\n",
        "        --model_dir=training/ \\\n",
        "        --pipeline_config_path=training/faster_rcnn_inception_v2_coco.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOe9FoUsH0kM"
      },
      "source": [
        "To monitor progress, refer to the tensorboard utility running in the cell prior to this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D59uRt1teARm"
      },
      "source": [
        "## Export the resulting model and use it to detect objects.\n",
        "In most cases, extracting a trained inference graph is necessary, as it can be used to perform object detection in deployment.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Steps:\n",
        "1. Copy the `models/research/object_detection/exporter_main_v2.py` script and paste it straight into the `object_detection/workspace/data` folder.\n",
        "\n",
        "2. Inside the `data/training` folder, find the ckpt-* checkpoint file with the highest number following the name of the dash e.g. model.ckpt-34350). This number represents the training step index at which the file was created.\n",
        "\n",
        "3. Make a note of the file’s name, as it will be passed as an argument when we call the `export_inference_graph.py` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2001o0IV3sw"
      },
      "source": [
        "# Copy the exporter script\n",
        "copyfile('models/research/object_detection/exporter_main_v2.py',\\\n",
        "         'workspace/data/exporter_main_v2.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSQrIb8genv3"
      },
      "source": [
        "# cd inside your data/ folder, and run the following command:\n",
        "!python workspace/data/exporter_main_v2.py \\\n",
        "    --input_type image_tensor \\\n",
        "    --pipeline_config_path training/faster_rcnn_inception_v2_coco.config \\\n",
        "    --trained_checkpoint_prefix training/model.ckpt-13302 \n",
        "    --output_directory trained-inference-graphs/output_inference_graph_v1.pb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAogz125gwMp"
      },
      "source": [
        "\n",
        "# References:\n",
        "1. [https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)"
      ]
    }
  ]
}